# PaperSeeker

## Introduction

PaperSeeker is an advanced academic paper search engine that uses AI to find, analyze, and present relevant research papers based on natural language queries. The system leverages Retrieval-Augmented Generation (RAG) to deliver more accurate and contextually relevant results than traditional keyword-based search engines.

Key features:
- Semantic search using vector embeddings (powered by Llama 3.2 model)
- Hybrid search combining vector similarity with BM25 text search for improved accuracy
- LLM-based reranking of search results to improve relevance
- Augmented generation of paper summaries and insights
- User-friendly web interface with chat-based interaction

## Project Structure

The project is divided into two main components:

### Backend (Python/FastAPI)

- **Vector Search Engine**: Handles semantic search using embeddings generated by Ollama's Llama 3.2 model
- **LLM Processor**: Performs reranking and generates augmented responses using the LLM
- **FastAPI Server**: Exposes REST endpoints for searching papers and retrieving results

```
backend/
├── app.py                 # Main FastAPI application server
├── vector_search.py       # Vector embedding and search functionality
├── llm_processor.py       # LLM-based reranking and response generation
├── models.py              # Pydantic models for API requests/responses
├── generate_embeddings.py # Script to generate embeddings for papers
├── requirements.txt       # Python dependencies
└── sample_papers.json     # Sample paper data for testing
```

### Frontend (Next.js)

- **Next.js App**: Modern React application with server components
- **Search Interface**: Interactive UI for querying papers
- **Results Display**: Responsive layout for showing search results and paper details

```
frontend/
├── app/                   # Next.js app folder
│   ├── components/        # React components for the UI
│   ├── api/               # API routes for client-server communication
│   ├── page.js            # Main page component
│   └── layout.js          # App layout component
├── public/                # Static assets
└── package.json           # JavaScript dependencies
```

## RAG Approach

PaperSeeker implements a sophisticated Retrieval-Augmented Generation (RAG) pipeline:

1. **Embedding Generation**:
   - Papers are processed to extract key information (title, abstract, authors, content)
   - Long papers are split into manageable chunks using RecursiveCharacterTextSplitter
   - Each chunk is independently converted to a vector embedding using Llama 3.2 model
   - The final paper embedding is calculated as the average of all chunk embeddings
   - These averaged embeddings are stored in Zilliz Cloud (Milvus) vector database

2. **Retrieval Process**:
   - User queries are converted to the same vector space as the papers
   - Vector similarity search finds semantically related papers
   - BM25 text search reranks papers from vector search

3. **Filtering with LLM**:
   - Top results from vector search and BM25 are passed to the LLM
   - Custom prompting guides the LLM to evaluate relevance to the query
   - Papers are filtered based on semantic understanding, not just keyword matches

4. **Augmentation Generation**:
   - LLM generates concise summaries of each paper's contribution to the query topic
   - Provides an introduction and conclusion that synthesizes findings across papers
   - Augmented content adds context and insights beyond what's explicitly in the papers

5. **Response Formatting**:
   - Results are structured as JSON with clearly defined sections
   - Frontend renders this in a user-friendly format with proper citations

## Running Instructions

### Prerequisites

- Python 3.8+ for the backend
- Node.js 16+ for the frontend
- Ollama for local LLM inference
- Zilliz Cloud account (or Milvus instance) for vector storage

### Backend Setup

1. Clone the repository and navigate to the backend directory:
   ```bash
   git clone https://github.com/yourusername/PaperSeeker.git
   cd PaperSeeker/backend
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   - Create a `.env` file in the backend directory with:
   ```
   ZILLIZ_CLOUD_URI=your_zilliz_cloud_endpoint
   ZILLIZ_CLOUD_TOKEN=your_zilliz_cloud_api_key
   ```

5. Generate embeddings for your papers:
   ```bash
   python generate_embeddings.py <paper data file>
   ```

6. Start the backend server:
   ```bash
   python app.py
   ```

### Frontend Setup

1. Navigate to the frontend directory:
   ```bash
   cd ../frontend
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Start the development server:
   ```bash
   npm run dev
   ```

4. Open your browser and navigate to http://localhost:3000

### Setting up Ollama

1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Pull the Llama 3.2 model:
   ```bash
   ollama pull llama3.2
   ```
3. Ensure Ollama is running before starting the backend

## Deploying to Production

For production deployment:

1. Set restrictive CORS settings in `app.py`
2. Configure proper environment variables for production
3. Use a production-grade server like Gunicorn for the backend
4. Build the frontend for production with `npm run build`
5. Deploy the frontend to a static hosting service or CDN

## Acknowledgments

This project uses several open-source libraries and models:
- langchain.text_splitter for paper text splitting 
- Llama 3.2 model from Meta
- Ollama for local model serving
- FastAPI for backend API
- Next.js for frontend
- Zilliz Cloud/Milvus for vector database

### Data Source

We used the peS2o dataset (https://github.com/allenai/peS2o) for academic papers. This dataset is a collection of ~40M open-access academic papers that have been cleaned, filtered, and formatted for pre-training language models.

## License

[MIT License](LICENSE)
